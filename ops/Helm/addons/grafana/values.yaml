# Common default values for grafana.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Docs: https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
grafana:
  enabled: true
  namespaceOverride: ""

  # Deploy default dashboards.
  ##
  defaultDashboardsEnabled: true

  adminPassword: shortlink

  grafana.ini:
    server:
      # this host can be localhost
      root_url: https://architecture.ddns.net/grafana

  serviceMonitor:
    enabled: true
    selfMonitor: true
    interval: "1m"

  imageRenderer:
    enabled: false

  ingress:
    enabled: true

    annotations:
      kubernetes.io/ingress.class: nginx
      cert-manager.io/cluster-issuer: cert-manager-production
      nginx.ingress.kubernetes.io/enable-modsecurity: "true"
      nginx.ingress.kubernetes.io/enable-owasp-core-rules: "true"
      nginx.ingress.kubernetes.io/enable-opentracing: "true"
      nginx.ingress.kubernetes.io/rewrite-target: /$1
      nginx.ingress.kubernetes.io/use-regex: "true"

    hosts:
      - architecture.ddns.net

    path: /grafana/?(.*)

    tls:
      - secretName: shortlink-ingress-tls
        hosts:
          - architecture.ddns.net

  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard

      # Annotations for Grafana dashboard configmaps
      ##
      annotations: {}
      multicluster: false
    datasources:
      enabled: true
      defaultDatasourceEnabled: true

      # Annotations for Grafana datasource configmaps
      ##
      annotations: {}

  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          uid: prometheus
          type: prometheus
          url: http://prometheus-operated.prometheus-operator:9090/prometheus
          access: proxy
          isDefault: true
          jsonData:
            timeInterval: 10s
            queryTimeout: 30s
            httpMethod: POST
            alertmanagerUid: alertmanager

        - name: Loki
          type: loki
          uid: loki
          access: proxy
          url: http://grafana-loki:3100
          jsonData:
            maxLines: 1000
            alertmanagerUid: alertmanager
            derivedFields:
              # Field with internal link pointing to data source in Grafana.
              # Right now, Grafana supports only Jaeger and Zipkin data sources as link targets.
              # datasourceUid value can be anything, but it should be unique across all defined data source uids.
              - datasourceUid: tempo
                matcherRegex: 'traceID":"(\w+)'
                name: TraceID
                # url will be interpreted as query for the datasource
                url: "$${__value.raw}"

              # Field with external link.
              - matcherRegex: 'traceID":"(\w+)'
                name: TraceID
                url: "http://grafana-tempo:16686/trace/$${__value.raw}"

        - name: Tempo
          type: tempo
          uid: tempo
          url: http://grafana-tempo:3100
          editable: false
          jsonData:
            lokiSearch:
              datasourceUid: loki
            nodeGraph:
              enabled: true
            serviceMap:
              datasourceUid: prometheus
            tracesToLogs:
              datasourceUid: loki
              filterBySpanID: true
              filterByTraceID: true
              mapTagNamesEnabled: true

        - name: Alertmanager
          type: alertmanager
          uid: alertmanager
          editable: false
          url: http://prometheus-alertmanager.prometheus-operator:9093
          readOnly: false
          jsonData:
            implementation: mimir

  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/default

  dashboards:
    default:
      cert-manager:
        # Ref: https://grafana.com/dashboards/11001
        gnetId: 11001
        revision: 1
        datasource: Prometheus
      node-exporter:
        gnetId: 1860
        revision: 21
        datasource: Prometheus
      ingress-nginx:
        url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/grafana/dashboards/nginx.json
      rabbitmq-overview:
        gnetId: 10991
        revision: 11
        datasource: Prometheus
      go-runtime:
        gnetId: 14061
        revision: 1
        datasource: Prometheus
      ceph:
        gnetId: 2842
        revision: 14
        datasource: Prometheus
      kyverno:
        url: https://raw.githubusercontent.com/kyverno/grafana-dashboard/master/grafana/dashboard.json

loki:
  enabled: true

promtail:
  enabled: true

  config:
    clients:
      - url: http://grafana-loki:3100/loki/api/v1/push

  extraScrapeConfigs:
    - job_name: syslog
      syslog:
        listen_address: 0.0.0.0:1514
        labels:
          job: "syslog"
    - job_name: journal
      journal:
        path: /var/log/journal
        max_age: 12h
        labels:
          job: systemd-journal
      relabel_configs:
        - source_labels: [ '__syslog_message_hostname' ]
          target_label: 'hostname'
        - source_labels: [ '__journal__systemd_unit' ]
          target_label: 'unit'
        - source_labels: [ '__journal__hostname' ]
          target_label: 'hostname'

  # Mount journal directory into promtail pods
  extraVolumes:
    - name: journal
      hostPath:
        path: /var/log/journal

  extraVolumeMounts:
    - name: journal
      mountPath: /var/log/journal
      readOnly: true

  syslogService:
    enabled: true
    type: LoadBalancer
    port: 1514

tempo:
  enabled: true

  tempo:
    searchEnabled: true
    metricsGenerator:
      enabled: true
      remoteWriteUrl: http://prometheus-operated.prometheus-operator:9090/api/v1/write

    ingester:
      trace_idle_period: 10s               # the length of time after a trace has not received spans to consider it complete and flush it
      max_block_bytes: 1_000_000           # cut the head block when it hits this size or ...
      max_block_duration: 5m               #   this much time passes

    querier:
      max_concurrent_queries: 100
      search:
        prefer_self: 50   # only if you're using external endpoints

    query_frontend:
      max_outstanding_per_tenant: 2000
      search:
        concurrent_jobs: 2000
        target_bytes_per_job: 400_000_000

    storage:
      trace:
        backend: local
        block:
          version: vParquet                # version of the block storage to use
          bloom_filter_false_positive: .05 # bloom filter false positive rate.  lower values create larger filters but fewer false positives
          index_downsample_bytes: 1000     # number of bytes per index record
          encoding: zstd                   # block encoding/compression.  options: none, gzip, lz4-64k, lz4-256k, lz4-1M, lz4, snappy, zstd
        local:
          path: /var/tempo/traces
        wal:
          path: /var/tempo/wal
        pool:
          max_workers: 100                 # the worker pool mainly drives querying, but is also used for polling the blocklist
          queue_depth: 10000

  serviceMonitor:
    enabled: true
